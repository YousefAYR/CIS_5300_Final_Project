{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biP3FKX-q4GL",
        "outputId": "c21664ab-59c1-42b5-e7d2-87b27c038336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Archive:  /content/CIS_5300_Final_Project-main.zip\n",
            "4c01898bf78ffdc539f20b411693d7fc1ab8d11b\n",
            "   creating: /content/CIS_5300_Final_Project-main/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/README.md  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/Simple_Baseline.ipynb  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/data.ipynb  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/data.md  \n",
            "   creating: /content/CIS_5300_Final_Project-main/data/\n",
            "   creating: /content/CIS_5300_Final_Project-main/data/2020_elections_results/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/data/2020_elections_results/president_county_candidate.csv  \n",
            "   creating: /content/CIS_5300_Final_Project-main/data/2020_tweets/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/data/2020_tweets/.gitignore  \n",
            "   creating: /content/CIS_5300_Final_Project-main/data/factoid_reddit/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/data/factoid_reddit/.gitignore  \n",
            "   creating: /content/CIS_5300_Final_Project-main/data/labeled_tweets_georgetown/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/data/labeled_tweets_georgetown/.gitignore  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/evaluate.py  \n",
            "   creating: /content/CIS_5300_Final_Project-main/raw_data/\n",
            "   creating: /content/CIS_5300_Final_Project-main/raw_data/2020_tweets/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/2020_tweets/.gitignore  \n",
            "   creating: /content/CIS_5300_Final_Project-main/raw_data/factoid_reddit/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/factoid_reddit/.gitignore  \n",
            "   creating: /content/CIS_5300_Final_Project-main/raw_data/labeled_tweets_georgetown/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/labeled_tweets_georgetown/dev.csv  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/labeled_tweets_georgetown/test.csv  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/labeled_tweets_georgetown/train.csv  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/scoring.md  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/simple-baseline.md  \n",
            "/content/CIS_5300_Final_Project-main\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Libraries (if needed)\n",
        "!pip install scikit-learn pandas numpy gdown\n",
        "\n",
        "# Step 2: Clone the Project Repository or Upload Data\n",
        "!unzip /content/CIS_5300_Final_Project-main.zip -d /content/\n",
        "\n",
        "# Step 3: Navigate to the Project Directory\n",
        "%cd /content/CIS_5300_Final_Project-main/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ],
      "metadata": {
        "id": "B0D8-tb5reMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Directories and Download Files\n",
        "# Create required directories if not already present\n",
        "os.makedirs('raw_data/labeled_tweets_georgetown/', exist_ok=True)\n",
        "os.makedirs('data/labeled_tweets_georgetown/', exist_ok=True)\n",
        "os.makedirs('raw_data/2020_tweets/', exist_ok=True)\n",
        "os.makedirs('raw_data/factoid_reddit/', exist_ok=True)\n",
        "\n",
        "# Download necessary files\n",
        "!gdown 1qrrznvcHkyUPoxq4GbauPxcT3mMGILVw  # archive.zip\n",
        "!gdown 1m_0tZXsqQSxaogvby83B5CRzgJcspFvU  # reddit_corpus_unbalanced_filtered.gzip\n",
        "\n",
        "# Move files to the correct directories\n",
        "!mv archive.zip raw_data/2020_tweets/\n",
        "!mv reddit_corpus_unbalanced_filtered.gzip raw_data/factoid_reddit/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPpkmGvXt-Bb",
        "outputId": "6c21b2c8-f5f2-4c8b-e68b-313341832a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1qrrznvcHkyUPoxq4GbauPxcT3mMGILVw\n",
            "From (redirected): https://drive.google.com/uc?id=1qrrznvcHkyUPoxq4GbauPxcT3mMGILVw&confirm=t&uuid=dba3a5b1-9b65-4455-a115-eb58e4becae9\n",
            "To: /content/CIS_5300_Final_Project-main/archive.zip\n",
            "100% 370M/370M [00:23<00:00, 15.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1m_0tZXsqQSxaogvby83B5CRzgJcspFvU\n",
            "From (redirected): https://drive.google.com/uc?id=1m_0tZXsqQSxaogvby83B5CRzgJcspFvU&confirm=t&uuid=4140ec9f-cd50-44cb-9e70-bce533ae4b86\n",
            "To: /content/CIS_5300_Final_Project-main/reddit_corpus_unbalanced_filtered.gzip\n",
            "100% 369M/369M [00:07<00:00, 49.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess Georgetown Labeled Tweets Dataset\n",
        "print(\"\\nProcessing Georgetown Labeled Tweets Dataset...\")\n",
        "raw_data_path = 'raw_data/labeled_tweets_georgetown/'\n",
        "output_path = 'data/labeled_tweets_georgetown/'\n",
        "\n",
        "# Define stance determination function\n",
        "def determine_stance(row):\n",
        "    if row['label'] == \"NONE\":\n",
        "        return 0\n",
        "    if (row['label'] == \"FAVOR\" and row['candidate'] == \"Trump\") or \\\n",
        "       (row['label'] == \"AGAINST\" and row['candidate'] == \"Biden\"):\n",
        "        return 1\n",
        "    if (row['label'] == \"FAVOR\" and row['candidate'] == \"Biden\") or \\\n",
        "       (row['label'] == \"AGAINST\" and row['candidate'] == \"Trump\"):\n",
        "        return -1\n",
        "\n",
        "# Process files in the labeled tweets directory\n",
        "for csv_file in os.listdir(raw_data_path):\n",
        "    if csv_file.endswith(\".csv\"):\n",
        "        file_path = os.path.join(raw_data_path, csv_file)\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        df['stance'] = df.apply(determine_stance, axis=1)\n",
        "        df = df.drop(columns=['tweet_id', 'label', 'candidate'])\n",
        "\n",
        "        output_file_path = os.path.join(output_path, csv_file)\n",
        "        df.to_csv(output_file_path, index=False)\n",
        "print(\"Georgetown dataset processing complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v00TTIT8rhex",
        "outputId": "91d94e02-fa55-4d3a-b003-6192f7723c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Georgetown Labeled Tweets Dataset...\n",
            "Georgetown dataset processing complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess FACTOID Reddit Dataset\n",
        "print(\"Processing FACTOID Reddit Dataset...\")\n",
        "raw_data_path = 'raw_data/factoid_reddit/'\n",
        "output_path = 'data/factoid_reddit/'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "df_raw = pd.read_pickle(raw_data_path + 'reddit_corpus_unbalanced_filtered.gzip', compression='gzip')\n",
        "\n",
        "# Keep only necessary columns\n",
        "columns_to_keep = ['documents', 'pb_factor']\n",
        "df_raw = df_raw[columns_to_keep]\n",
        "\n",
        "# Transform into text and stance columns\n",
        "df = pd.DataFrame({\n",
        "    \"text\": df_raw[\"documents\"].apply(lambda x: [tup[1] for tup in x]),\n",
        "    \"stance\": df_raw[\"pb_factor\"].apply(lambda x: -1 if x < -0.5 else (1 if x > 0.5 else 0))\n",
        "})\n",
        "df = df.explode(\"text\").reset_index(drop=True)\n",
        "\n",
        "# Split data into train, dev, and test sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save processed datasets\n",
        "train_df.to_csv(output_path + 'train.csv', index=False)\n",
        "dev_df.to_csv(output_path + 'dev.csv', index=False)\n",
        "test_df.to_csv(output_path + 'test.csv', index=False)\n",
        "print(\"FACTOID dataset processing complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6iau2TGuPab",
        "outputId": "0c4f4ff2-169b-4aa5-d3d6-798784e8b8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing FACTOID Reddit Dataset...\n",
            "FACTOID dataset processing complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess 2020 Tweets Dataset\n",
        "print(\"Processing 2020 Tweets Dataset...\")\n",
        "!unzip raw_data/2020_tweets/archive.zip -d raw_data/2020_tweets/\n",
        "raw_data_path = 'raw_data/2020_tweets/'\n",
        "output_path = 'data/2020_tweets/'\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "df_biden = pd.read_csv(raw_data_path + 'hashtag_joebiden.csv', lineterminator='\\n')\n",
        "df_trump = pd.read_csv(raw_data_path + 'hashtag_donaldtrump.csv', lineterminator='\\n')\n",
        "\n",
        "# Remove duplicates and sort by date\n",
        "df_biden['created_at'] = pd.to_datetime(df_biden['created_at'])\n",
        "df_biden = df_biden.sort_values(by='created_at', ascending=False).drop_duplicates(subset='tweet_id')\n",
        "\n",
        "df_trump['created_at'] = pd.to_datetime(df_trump['created_at'])\n",
        "df_trump = df_trump.sort_values(by='created_at', ascending=False).drop_duplicates(subset='tweet_id')\n",
        "\n",
        "# Add candidate labels\n",
        "df_biden['contains'] = \"Biden\"\n",
        "df_trump['contains'] = \"Trump\"\n",
        "\n",
        "# Merge datasets and clean\n",
        "df = pd.concat([df_biden, df_trump])\n",
        "df['contains'] = df.groupby('tweet_id')['contains'].transform(\n",
        "    lambda x: 'Both' if len(set(x)) > 1 else x\n",
        ")\n",
        "df = df.drop_duplicates(subset='tweet_id').reset_index(drop=True)\n",
        "\n",
        "# Split data into train, dev, and test sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save processed datasets\n",
        "train_df.to_csv(output_path + 'train.csv', index=False)\n",
        "dev_df.to_csv(output_path + 'dev.csv', index=False)\n",
        "test_df.to_csv(output_path + 'test.csv', index=False)\n",
        "print(\"2020 Tweets dataset processing complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqVUFAHmq_PA",
        "outputId": "3123f61e-a2bc-41c1-dd60-ffd740f318ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2020 Tweets Dataset...\n",
            "Archive:  raw_data/2020_tweets/archive.zip\n",
            "replace raw_data/2020_tweets/hashtag_donaldtrump.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: raw_data/2020_tweets/hashtag_donaldtrump.csv  \n",
            "replace raw_data/2020_tweets/hashtag_joebiden.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: raw_data/2020_tweets/hashtag_joebiden.csv  \n",
            "2020 Tweets dataset processing complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Processed Data\n",
        "print(\"Processed files:\")\n",
        "print(os.listdir('data/labeled_tweets_georgetown/'))\n",
        "print(os.listdir('data/factoid_reddit/'))\n",
        "print(os.listdir('data/2020_tweets/'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjiCTL_PrMZD",
        "outputId": "85b0a70b-e992-454c-94fd-c8bbf357beaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed files:\n",
            "['train.csv', 'dev.csv', '.gitignore', 'test.csv']\n",
            "['train.csv', 'dev.csv', '.gitignore', 'test.csv']\n",
            "['.gitignore']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "tIvQ5UyNtaNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "factoid_train = pd.read_csv('data/factoid_reddit/train.csv', on_bad_lines='skip', engine='python')\n",
        "factoid_dev = pd.read_csv('data/factoid_reddit/dev.csv', on_bad_lines='skip', engine='python')\n",
        "factoid_test = pd.read_csv('data/factoid_reddit/test.csv', on_bad_lines='skip', engine='python')\n",
        "\n",
        "georgetown_train = pd.read_csv('data/labeled_tweets_georgetown/train.csv', on_bad_lines='skip', engine='python')\n",
        "georgetown_dev = pd.read_csv('data/labeled_tweets_georgetown/dev.csv', on_bad_lines='skip', engine='python')\n",
        "georgetown_test = pd.read_csv('data/labeled_tweets_georgetown/test.csv', on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Check for NaNs and fill them with empty strings\n",
        "for dataset in [factoid_train, factoid_dev, factoid_test, georgetown_train, georgetown_dev, georgetown_test]:\n",
        "    dataset['text'] = dataset['text'].fillna(\"\")"
      ],
      "metadata": {
        "id": "SsL-f-jH25Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_baseline_model(train_data, dev_data, test_data, vectorizer=None):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a Logistic Regression baseline model.\n",
        "\n",
        "    Args:\n",
        "    - train_data, dev_data, test_data: DataFrames with 'text' and 'stance' columns.\n",
        "    - vectorizer: Optional TfidfVectorizer instance for text vectorization.\n",
        "\n",
        "    Returns:\n",
        "    - Logistic Regression model and evaluation results for test data.\n",
        "    \"\"\"\n",
        "    # Split into inputs and labels\n",
        "    X_train, y_train = train_data['text'], train_data['stance']\n",
        "    X_dev, y_dev = dev_data['text'], dev_data['stance']\n",
        "    X_test, y_test = test_data['text'], test_data['stance']\n",
        "\n",
        "    # Vectorize text\n",
        "    if vectorizer is None:\n",
        "        vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 1))\n",
        "        X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    else:\n",
        "        X_train_vec = vectorizer.transform(X_train)\n",
        "\n",
        "    X_dev_vec = vectorizer.transform(X_dev)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train a Logistic Regression baseline\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluate on dev and test sets\n",
        "    y_dev_pred = model.predict(X_dev_vec)\n",
        "    y_test_pred = model.predict(X_test_vec)\n",
        "\n",
        "    return model, vectorizer, y_test, y_test_pred"
      ],
      "metadata": {
        "id": "jNEnm3U44YXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factoid_train = factoid_train.dropna(subset=['stance'])\n",
        "factoid_dev = factoid_dev.dropna(subset=['stance'])\n",
        "factoid_test = factoid_test.dropna(subset=['stance'])"
      ],
      "metadata": {
        "id": "UsBPTg8r-rro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaNs in the stance column\n",
        "print(factoid_train['stance'].isna().sum())\n",
        "print(factoid_dev['stance'].isna().sum())\n",
        "print(factoid_test['stance'].isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr37uD2_-nCD",
        "outputId": "aa55c6f2-db58-47eb-da42-1b310b9bdbfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "factoid_model, factoid_vectorizer, factoid_y_test, factoid_y_test_pred = train_baseline_model(\n",
        "    factoid_train, factoid_dev, factoid_test\n",
        ")"
      ],
      "metadata": {
        "id": "yuT5mkVK5-w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available!\")\n",
        "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU is not available. Check your environment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wDqpxI-WyAy",
        "outputId": "220cc38b-42cf-4b88-e6c2-bebf625b367e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available!\n",
            "Device Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "georgetown_train = georgetown_train.dropna(subset=['stance'])\n",
        "georgetown_dev = georgetown_dev.dropna(subset=['stance'])\n",
        "georgetown_test = georgetown_test.dropna(subset=['stance'])"
      ],
      "metadata": {
        "id": "4sngRSlNXNQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaNs in the stance column\n",
        "print(georgetown_train['stance'].isna().sum())\n",
        "print(georgetown_dev['stance'].isna().sum())\n",
        "print(georgetown_test['stance'].isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lp9PjAQXEtN",
        "outputId": "d959a6b8-c99a-485a-f10a-70d5ebf9ba09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "georgetown_model, georgetown_vectorizer, georgetown_y_test, georgetown_y_test_pred = train_baseline_model(\n",
        "    georgetown_train, georgetown_dev, georgetown_test\n",
        ")"
      ],
      "metadata": {
        "id": "L0xN6acS5-U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import evaluate_model, display_evaluation_results\n",
        "\n",
        "# FACTOID Evaluation\n",
        "factoid_labels = sorted(set(factoid_y_test))  # Dynamically get labels from y_test\n",
        "factoid_results = evaluate_model(factoid_y_test, factoid_y_test_pred, factoid_labels)\n",
        "factoid_output_file = \"factoid_evaluation_results.txt\"\n",
        "display_evaluation_results(factoid_results, factoid_labels, factoid_output_file)\n",
        "\n",
        "# Read and display FACTOID evaluation results\n",
        "print(\"FACTOID Dataset Evaluation:\")\n",
        "with open(factoid_output_file, 'r') as f:\n",
        "    print(f.read())\n",
        "\n",
        "# Georgetown Evaluation\n",
        "georgetown_labels = sorted(set(georgetown_y_test))  # Dynamically get labels from y_test\n",
        "georgetown_results = evaluate_model(georgetown_y_test, georgetown_y_test_pred, georgetown_labels)\n",
        "georgetown_output_file = \"georgetown_evaluation_results.txt\"\n",
        "display_evaluation_results(georgetown_results, georgetown_labels, georgetown_output_file)\n",
        "\n",
        "# Read and display Georgetown evaluation results\n",
        "print(\"\\nGeorgetown Dataset Evaluation:\")\n",
        "with open(georgetown_output_file, 'r') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vcGp_G9XD7p",
        "outputId": "c1c27331-6bd7-46a8-b802-bf67c7dd4bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FACTOID Dataset Evaluation:\n",
            "Model Evaluation Metrics\n",
            "-------------------------\n",
            "Accuracy: 0.5751\n",
            "Macro Precision: 0.4966\n",
            "Macro Recall: 0.3589\n",
            "Macro F1-Score: 0.3106\n",
            "\n",
            "Per-Class Metrics:\n",
            "Class -1.0: Precision=0.5831, Recall=0.9529, F1-Score=0.7235\n",
            "Class 0.0: Precision=0.4868, Recall=0.1062, F1-Score=0.1743\n",
            "Class 1.0: Precision=0.4197, Recall=0.0177, F1-Score=0.0340\n",
            "\n",
            "Confusion Matrix:\n",
            "        -1.0    0.0   1.0\n",
            "-1.0  179939   8606   291\n",
            " 0.0  104606  12473   421\n",
            " 1.0   24053   4541   515\n",
            "\n",
            "\n",
            "Georgetown Dataset Evaluation:\n",
            "Model Evaluation Metrics\n",
            "-------------------------\n",
            "Accuracy: 0.6788\n",
            "Macro Precision: 0.6919\n",
            "Macro Recall: 0.5711\n",
            "Macro F1-Score: 0.6009\n",
            "\n",
            "Per-Class Metrics:\n",
            "Class -1.0: Precision=0.7368, Recall=0.4118, F1-Score=0.5283\n",
            "Class 0.0: Precision=0.6721, Recall=0.8913, F1-Score=0.7664\n",
            "Class 1.0: Precision=0.6667, Recall=0.4103, F1-Score=0.5079\n",
            "\n",
            "Confusion Matrix:\n",
            "      -1.0   0.0   1.0\n",
            "-1.0    14    18     2\n",
            " 0.0     4    82     6\n",
            " 1.0     1    22    16\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1bUyEjm96Rb5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
