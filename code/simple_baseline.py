# -*- coding: utf-8 -*-
"""Simple_Baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19SMIPgAI7iTttlTdcQKIi1wpJgSLKqPf
"""

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Install Required Libraries (if needed)
!pip install scikit-learn pandas numpy gdown

# Step 2: Clone the Project Repository or Upload Data
!unzip /content/CIS_5300_Final_Project-main.zip -d /content/

# Step 3: Navigate to the Project Directory
# %cd /content/CIS_5300_Final_Project-main/

import pandas as pd
from sklearn.model_selection import train_test_split
import os

# Setup Directories and Download Files
# Create required directories if not already present
os.makedirs('raw_data/labeled_tweets_georgetown/', exist_ok=True)
os.makedirs('data/labeled_tweets_georgetown/', exist_ok=True)
os.makedirs('raw_data/2020_tweets/', exist_ok=True)
os.makedirs('raw_data/factoid_reddit/', exist_ok=True)

# Download necessary files
!gdown 1qrrznvcHkyUPoxq4GbauPxcT3mMGILVw  # archive.zip
!gdown 1m_0tZXsqQSxaogvby83B5CRzgJcspFvU  # reddit_corpus_unbalanced_filtered.gzip

# Move files to the correct directories
!mv archive.zip raw_data/2020_tweets/
!mv reddit_corpus_unbalanced_filtered.gzip raw_data/factoid_reddit/

# Preprocess Georgetown Labeled Tweets Dataset
print("\nProcessing Georgetown Labeled Tweets Dataset...")
raw_data_path = 'raw_data/labeled_tweets_georgetown/'
output_path = 'data/labeled_tweets_georgetown/'

# Define stance determination function
def determine_stance(row):
    if row['label'] == "NONE":
        return 0
    if (row['label'] == "FAVOR" and row['candidate'] == "Trump") or \
       (row['label'] == "AGAINST" and row['candidate'] == "Biden"):
        return 1
    if (row['label'] == "FAVOR" and row['candidate'] == "Biden") or \
       (row['label'] == "AGAINST" and row['candidate'] == "Trump"):
        return -1

# Process files in the labeled tweets directory
for csv_file in os.listdir(raw_data_path):
    if csv_file.endswith(".csv"):
        file_path = os.path.join(raw_data_path, csv_file)
        df = pd.read_csv(file_path)

        df['stance'] = df.apply(determine_stance, axis=1)
        df = df.drop(columns=['tweet_id', 'label', 'candidate'])

        output_file_path = os.path.join(output_path, csv_file)
        df.to_csv(output_file_path, index=False)
print("Georgetown dataset processing complete.\n")

# Preprocess FACTOID Reddit Dataset
print("Processing FACTOID Reddit Dataset...")
raw_data_path = 'raw_data/factoid_reddit/'
output_path = 'data/factoid_reddit/'
os.makedirs(output_path, exist_ok=True)

df_raw = pd.read_pickle(raw_data_path + 'reddit_corpus_unbalanced_filtered.gzip', compression='gzip')

# Keep only necessary columns
columns_to_keep = ['documents', 'pb_factor']
df_raw = df_raw[columns_to_keep]

# Transform into text and stance columns
df = pd.DataFrame({
    "text": df_raw["documents"].apply(lambda x: [tup[1] for tup in x]),
    "stance": df_raw["pb_factor"].apply(lambda x: -1 if x < -0.5 else (1 if x > 0.5 else 0))
})
df = df.explode("text").reset_index(drop=True)

# Split data into train, dev, and test sets
train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)
dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

# Save processed datasets
train_df.to_csv(output_path + 'train.csv', index=False)
dev_df.to_csv(output_path + 'dev.csv', index=False)
test_df.to_csv(output_path + 'test.csv', index=False)
print("FACTOID dataset processing complete.\n")

# Preprocess 2020 Tweets Dataset
print("Processing 2020 Tweets Dataset...")
!unzip raw_data/2020_tweets/archive.zip -d raw_data/2020_tweets/
raw_data_path = 'raw_data/2020_tweets/'
output_path = 'data/2020_tweets/'

# Ensure the output directory exists
os.makedirs(output_path, exist_ok=True)

# Load data
df_biden = pd.read_csv(raw_data_path + 'hashtag_joebiden.csv', lineterminator='\n')
df_trump = pd.read_csv(raw_data_path + 'hashtag_donaldtrump.csv', lineterminator='\n')

# Remove duplicates and sort by date
df_biden['created_at'] = pd.to_datetime(df_biden['created_at'])
df_biden = df_biden.sort_values(by='created_at', ascending=False).drop_duplicates(subset='tweet_id')

df_trump['created_at'] = pd.to_datetime(df_trump['created_at'])
df_trump = df_trump.sort_values(by='created_at', ascending=False).drop_duplicates(subset='tweet_id')

# Add candidate labels
df_biden['contains'] = "Biden"
df_trump['contains'] = "Trump"

# Merge datasets and clean
df = pd.concat([df_biden, df_trump])
df['contains'] = df.groupby('tweet_id')['contains'].transform(
    lambda x: 'Both' if len(set(x)) > 1 else x
)
df = df.drop_duplicates(subset='tweet_id').reset_index(drop=True)

# Split data into train, dev, and test sets
train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)
dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

# Save processed datasets
train_df.to_csv(output_path + 'train.csv', index=False)
dev_df.to_csv(output_path + 'dev.csv', index=False)
test_df.to_csv(output_path + 'test.csv', index=False)
print("2020 Tweets dataset processing complete.\n")

# Verify Processed Data
print("Processed files:")
print(os.listdir('data/labeled_tweets_georgetown/'))
print(os.listdir('data/factoid_reddit/'))
print(os.listdir('data/2020_tweets/'))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load datasets
factoid_train = pd.read_csv('data/factoid_reddit/train.csv', on_bad_lines='skip', engine='python')
factoid_dev = pd.read_csv('data/factoid_reddit/dev.csv', on_bad_lines='skip', engine='python')
factoid_test = pd.read_csv('data/factoid_reddit/test.csv', on_bad_lines='skip', engine='python')

georgetown_train = pd.read_csv('data/labeled_tweets_georgetown/train.csv', on_bad_lines='skip', engine='python')
georgetown_dev = pd.read_csv('data/labeled_tweets_georgetown/dev.csv', on_bad_lines='skip', engine='python')
georgetown_test = pd.read_csv('data/labeled_tweets_georgetown/test.csv', on_bad_lines='skip', engine='python')

# Check for NaNs and fill them with empty strings
for dataset in [factoid_train, factoid_dev, factoid_test, georgetown_train, georgetown_dev, georgetown_test]:
    dataset['text'] = dataset['text'].fillna("")

def train_baseline_model(train_data, dev_data, test_data, vectorizer=None):
    """
    Trains and evaluates a Logistic Regression baseline model.

    Args:
    - train_data, dev_data, test_data: DataFrames with 'text' and 'stance' columns.
    - vectorizer: Optional TfidfVectorizer instance for text vectorization.

    Returns:
    - Logistic Regression model and evaluation results for test data.
    """
    # Split into inputs and labels
    X_train, y_train = train_data['text'], train_data['stance']
    X_dev, y_dev = dev_data['text'], dev_data['stance']
    X_test, y_test = test_data['text'], test_data['stance']

    # Vectorize text
    if vectorizer is None:
        vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 1))
        X_train_vec = vectorizer.fit_transform(X_train)
    else:
        X_train_vec = vectorizer.transform(X_train)

    X_dev_vec = vectorizer.transform(X_dev)
    X_test_vec = vectorizer.transform(X_test)

    # Train a Logistic Regression baseline
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train_vec, y_train)

    # Evaluate on dev and test sets
    y_dev_pred = model.predict(X_dev_vec)
    y_test_pred = model.predict(X_test_vec)

    return model, vectorizer, y_test, y_test_pred

factoid_train = factoid_train.dropna(subset=['stance'])
factoid_dev = factoid_dev.dropna(subset=['stance'])
factoid_test = factoid_test.dropna(subset=['stance'])

# Check for NaNs in the stance column
print(factoid_train['stance'].isna().sum())
print(factoid_dev['stance'].isna().sum())
print(factoid_test['stance'].isna().sum())

factoid_model, factoid_vectorizer, factoid_y_test, factoid_y_test_pred = train_baseline_model(
    factoid_train, factoid_dev, factoid_test
)

import torch

if torch.cuda.is_available():
    print("GPU is available!")
    print(f"Device Name: {torch.cuda.get_device_name(0)}")
else:
    print("GPU is not available. Check your environment.")

georgetown_train = georgetown_train.dropna(subset=['stance'])
georgetown_dev = georgetown_dev.dropna(subset=['stance'])
georgetown_test = georgetown_test.dropna(subset=['stance'])

# Check for NaNs in the stance column
print(georgetown_train['stance'].isna().sum())
print(georgetown_dev['stance'].isna().sum())
print(georgetown_test['stance'].isna().sum())

georgetown_model, georgetown_vectorizer, georgetown_y_test, georgetown_y_test_pred = train_baseline_model(
    georgetown_train, georgetown_dev, georgetown_test
)

from evaluate import evaluate_model, display_evaluation_results

# FACTOID Evaluation
factoid_labels = sorted(set(factoid_y_test))  # Dynamically get labels from y_test
factoid_results = evaluate_model(factoid_y_test, factoid_y_test_pred, factoid_labels)
factoid_output_file = "factoid_evaluation_results.txt"
display_evaluation_results(factoid_results, factoid_labels, factoid_output_file)

# Read and display FACTOID evaluation results
print("FACTOID Dataset Evaluation:")
with open(factoid_output_file, 'r') as f:
    print(f.read())

# Georgetown Evaluation
georgetown_labels = sorted(set(georgetown_y_test))  # Dynamically get labels from y_test
georgetown_results = evaluate_model(georgetown_y_test, georgetown_y_test_pred, georgetown_labels)
georgetown_output_file = "georgetown_evaluation_results.txt"
display_evaluation_results(georgetown_results, georgetown_labels, georgetown_output_file)

# Read and display Georgetown evaluation results
print("\nGeorgetown Dataset Evaluation:")
with open(georgetown_output_file, 'r') as f:
    print(f.read())

