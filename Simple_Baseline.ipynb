{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biP3FKX-q4GL",
        "outputId": "e351ff0e-2320-42f2-c7d1-70d9bc6c9025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Archive:  /content/CIS_5300_Final_Project-main.zip\n",
            "a7b7c9f4fb585aed2686964bbedbd23618ba4e71\n",
            "   creating: /content/CIS_5300_Final_Project-main/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/README.md  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/data.ipynb  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/data.md  \n",
            "   creating: /content/CIS_5300_Final_Project-main/data/\n",
            "   creating: /content/CIS_5300_Final_Project-main/data/2020_elections_results/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/data/2020_elections_results/president_county_candidate.csv  \n",
            "   creating: /content/CIS_5300_Final_Project-main/data/2020_tweets/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/data/2020_tweets/.gitignore  \n",
            "   creating: /content/CIS_5300_Final_Project-main/data/factoid_reddit/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/data/factoid_reddit/.gitignore  \n",
            "   creating: /content/CIS_5300_Final_Project-main/data/labeled_tweets_georgetown/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/data/labeled_tweets_georgetown/.gitignore  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/evaluate.py  \n",
            "   creating: /content/CIS_5300_Final_Project-main/raw_data/\n",
            "   creating: /content/CIS_5300_Final_Project-main/raw_data/2020_tweets/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/2020_tweets/.gitignore  \n",
            "   creating: /content/CIS_5300_Final_Project-main/raw_data/factoid_reddit/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/factoid_reddit/.gitignore  \n",
            "   creating: /content/CIS_5300_Final_Project-main/raw_data/labeled_tweets_georgetown/\n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/labeled_tweets_georgetown/dev.csv  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/labeled_tweets_georgetown/test.csv  \n",
            "  inflating: /content/CIS_5300_Final_Project-main/raw_data/labeled_tweets_georgetown/train.csv  \n",
            "/content/CIS_5300_Final_Project-main\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Libraries (if needed)\n",
        "!pip install scikit-learn pandas numpy gdown\n",
        "\n",
        "# Step 2: Clone the Project Repository or Upload Data\n",
        "!unzip /content/CIS_5300_Final_Project-main.zip -d /content/\n",
        "\n",
        "# Step 3: Navigate to the Project Directory\n",
        "%cd /content/CIS_5300_Final_Project-main/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ],
      "metadata": {
        "id": "B0D8-tb5reMp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Directories and Download Files\n",
        "# Create required directories if not already present\n",
        "os.makedirs('raw_data/labeled_tweets_georgetown/', exist_ok=True)\n",
        "os.makedirs('data/labeled_tweets_georgetown/', exist_ok=True)\n",
        "os.makedirs('raw_data/2020_tweets/', exist_ok=True)\n",
        "os.makedirs('raw_data/factoid_reddit/', exist_ok=True)\n",
        "\n",
        "# Download necessary files\n",
        "!gdown 1qrrznvcHkyUPoxq4GbauPxcT3mMGILVw  # archive.zip\n",
        "!gdown 1m_0tZXsqQSxaogvby83B5CRzgJcspFvU  # reddit_corpus_unbalanced_filtered.gzip\n",
        "\n",
        "# Move files to the correct directories\n",
        "!mv archive.zip raw_data/2020_tweets/\n",
        "!mv reddit_corpus_unbalanced_filtered.gzip raw_data/factoid_reddit/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPpkmGvXt-Bb",
        "outputId": "0dd8e3c4-1732-4965-ee88-7a65b02e901c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1qrrznvcHkyUPoxq4GbauPxcT3mMGILVw\n",
            "From (redirected): https://drive.google.com/uc?id=1qrrznvcHkyUPoxq4GbauPxcT3mMGILVw&confirm=t&uuid=398ab4c0-63f9-433a-bfaa-bd0dea1d7b5c\n",
            "To: /content/CIS_5300_Final_Project-main/archive.zip\n",
            "100% 370M/370M [00:02<00:00, 140MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1m_0tZXsqQSxaogvby83B5CRzgJcspFvU\n",
            "From (redirected): https://drive.google.com/uc?id=1m_0tZXsqQSxaogvby83B5CRzgJcspFvU&confirm=t&uuid=abe9bac8-8b84-439c-b905-8e2c4b64bde4\n",
            "To: /content/CIS_5300_Final_Project-main/reddit_corpus_unbalanced_filtered.gzip\n",
            "100% 369M/369M [00:02<00:00, 162MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess Georgetown Labeled Tweets Dataset\n",
        "print(\"\\nProcessing Georgetown Labeled Tweets Dataset...\")\n",
        "raw_data_path = 'raw_data/labeled_tweets_georgetown/'\n",
        "output_path = 'data/labeled_tweets_georgetown/'\n",
        "\n",
        "# Define stance determination function\n",
        "def determine_stance(row):\n",
        "    if row['label'] == \"NONE\":\n",
        "        return 0\n",
        "    if (row['label'] == \"FAVOR\" and row['candidate'] == \"Trump\") or \\\n",
        "       (row['label'] == \"AGAINST\" and row['candidate'] == \"Biden\"):\n",
        "        return 1\n",
        "    if (row['label'] == \"FAVOR\" and row['candidate'] == \"Biden\") or \\\n",
        "       (row['label'] == \"AGAINST\" and row['candidate'] == \"Trump\"):\n",
        "        return -1\n",
        "\n",
        "# Process files in the labeled tweets directory\n",
        "for csv_file in os.listdir(raw_data_path):\n",
        "    if csv_file.endswith(\".csv\"):\n",
        "        file_path = os.path.join(raw_data_path, csv_file)\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        df['stance'] = df.apply(determine_stance, axis=1)\n",
        "        df = df.drop(columns=['tweet_id', 'label', 'candidate'])\n",
        "\n",
        "        output_file_path = os.path.join(output_path, csv_file)\n",
        "        df.to_csv(output_file_path, index=False)\n",
        "print(\"Georgetown dataset processing complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v00TTIT8rhex",
        "outputId": "7f8bba52-4076-47ab-da3c-a48147de1adb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Georgetown Labeled Tweets Dataset...\n",
            "Georgetown dataset processing complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess FACTOID Reddit Dataset\n",
        "print(\"Processing FACTOID Reddit Dataset...\")\n",
        "raw_data_path = 'raw_data/factoid_reddit/'\n",
        "output_path = 'data/factoid_reddit/'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "df_raw = pd.read_pickle(raw_data_path + 'reddit_corpus_unbalanced_filtered.gzip', compression='gzip')\n",
        "\n",
        "# Keep only necessary columns\n",
        "columns_to_keep = ['documents', 'pb_factor']\n",
        "df_raw = df_raw[columns_to_keep]\n",
        "\n",
        "# Transform into text and stance columns\n",
        "df = pd.DataFrame({\n",
        "    \"text\": df_raw[\"documents\"].apply(lambda x: [tup[1] for tup in x]),\n",
        "    \"stance\": df_raw[\"pb_factor\"].apply(lambda x: -1 if x < -0.5 else (1 if x > 0.5 else 0))\n",
        "})\n",
        "df = df.explode(\"text\").reset_index(drop=True)\n",
        "\n",
        "# Split data into train, dev, and test sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save processed datasets\n",
        "train_df.to_csv(output_path + 'train.csv', index=False)\n",
        "dev_df.to_csv(output_path + 'dev.csv', index=False)\n",
        "test_df.to_csv(output_path + 'test.csv', index=False)\n",
        "print(\"FACTOID dataset processing complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6iau2TGuPab",
        "outputId": "95278302-6fd2-4bd5-bd0c-9f20c7892c14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing FACTOID Reddit Dataset...\n",
            "FACTOID dataset processing complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess 2020 Tweets Dataset\n",
        "print(\"Processing 2020 Tweets Dataset...\")\n",
        "!unzip raw_data/2020_tweets/archive.zip -d raw_data/2020_tweets/\n",
        "raw_data_path = 'raw_data/2020_tweets/'\n",
        "output_path = 'data/2020_tweets/'\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "df_biden = pd.read_csv(raw_data_path + 'hashtag_joebiden.csv', lineterminator='\\n')\n",
        "df_trump = pd.read_csv(raw_data_path + 'hashtag_donaldtrump.csv', lineterminator='\\n')\n",
        "\n",
        "# Remove duplicates and sort by date\n",
        "df_biden['created_at'] = pd.to_datetime(df_biden['created_at'])\n",
        "df_biden = df_biden.sort_values(by='created_at', ascending=False).drop_duplicates(subset='tweet_id')\n",
        "\n",
        "df_trump['created_at'] = pd.to_datetime(df_trump['created_at'])\n",
        "df_trump = df_trump.sort_values(by='created_at', ascending=False).drop_duplicates(subset='tweet_id')\n",
        "\n",
        "# Add candidate labels\n",
        "df_biden['contains'] = \"Biden\"\n",
        "df_trump['contains'] = \"Trump\"\n",
        "\n",
        "# Merge datasets and clean\n",
        "df = pd.concat([df_biden, df_trump])\n",
        "df['contains'] = df.groupby('tweet_id')['contains'].transform(\n",
        "    lambda x: 'Both' if len(set(x)) > 1 else x\n",
        ")\n",
        "df = df.drop_duplicates(subset='tweet_id').reset_index(drop=True)\n",
        "\n",
        "# Split data into train, dev, and test sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save processed datasets\n",
        "train_df.to_csv(output_path + 'train.csv', index=False)\n",
        "dev_df.to_csv(output_path + 'dev.csv', index=False)\n",
        "test_df.to_csv(output_path + 'test.csv', index=False)\n",
        "print(\"2020 Tweets dataset processing complete.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqVUFAHmq_PA",
        "outputId": "3123f61e-a2bc-41c1-dd60-ffd740f318ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 2020 Tweets Dataset...\n",
            "Archive:  raw_data/2020_tweets/archive.zip\n",
            "replace raw_data/2020_tweets/hashtag_donaldtrump.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: raw_data/2020_tweets/hashtag_donaldtrump.csv  \n",
            "replace raw_data/2020_tweets/hashtag_joebiden.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: raw_data/2020_tweets/hashtag_joebiden.csv  \n",
            "2020 Tweets dataset processing complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify Processed Data\n",
        "print(\"Processed files:\")\n",
        "print(os.listdir('data/labeled_tweets_georgetown/'))\n",
        "print(os.listdir('data/factoid_reddit/'))\n",
        "print(os.listdir('data/2020_tweets/'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjiCTL_PrMZD",
        "outputId": "48901b8b-5f66-4c71-c277-d98cbb148e21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed files:\n",
            "['train.csv', 'dev.csv', '.gitignore', 'test.csv']\n",
            "['train.csv', 'dev.csv', '.gitignore', 'test.csv']\n",
            "['.gitignore']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "tIvQ5UyNtaNB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "factoid_train = pd.read_csv('data/factoid_reddit/train.csv', on_bad_lines='skip', engine='python')\n",
        "factoid_dev = pd.read_csv('data/factoid_reddit/dev.csv', on_bad_lines='skip', engine='python')\n",
        "factoid_test = pd.read_csv('data/factoid_reddit/test.csv', on_bad_lines='skip', engine='python')\n",
        "\n",
        "georgetown_train = pd.read_csv('data/labeled_tweets_georgetown/train.csv', on_bad_lines='skip', engine='python')\n",
        "georgetown_dev = pd.read_csv('data/labeled_tweets_georgetown/dev.csv', on_bad_lines='skip', engine='python')\n",
        "georgetown_test = pd.read_csv('data/labeled_tweets_georgetown/test.csv', on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Check for NaNs and fill them with empty strings\n",
        "for dataset in [factoid_train, factoid_dev, factoid_test, georgetown_train, georgetown_dev, georgetown_test]:\n",
        "    dataset['text'] = dataset['text'].fillna(\"\")"
      ],
      "metadata": {
        "id": "SsL-f-jH25Al"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_baseline_model(train_data, dev_data, test_data, vectorizer=None):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a Logistic Regression baseline model.\n",
        "\n",
        "    Args:\n",
        "    - train_data, dev_data, test_data: DataFrames with 'text' and 'stance' columns.\n",
        "    - vectorizer: Optional TfidfVectorizer instance for text vectorization.\n",
        "\n",
        "    Returns:\n",
        "    - Logistic Regression model and evaluation results for test data.\n",
        "    \"\"\"\n",
        "    # Split into inputs and labels\n",
        "    X_train, y_train = train_data['text'], train_data['stance']\n",
        "    X_dev, y_dev = dev_data['text'], dev_data['stance']\n",
        "    X_test, y_test = test_data['text'], test_data['stance']\n",
        "\n",
        "    # Vectorize text\n",
        "    if vectorizer is None:\n",
        "        vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 1))\n",
        "        X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    else:\n",
        "        X_train_vec = vectorizer.transform(X_train)\n",
        "\n",
        "    X_dev_vec = vectorizer.transform(X_dev)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train a Logistic Regression baseline\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluate on dev and test sets\n",
        "    y_dev_pred = model.predict(X_dev_vec)\n",
        "    y_test_pred = model.predict(X_test_vec)\n",
        "\n",
        "    return model, vectorizer, y_test, y_test_pred"
      ],
      "metadata": {
        "id": "jNEnm3U44YXs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factoid_train = factoid_train.dropna(subset=['stance'])\n",
        "factoid_dev = factoid_dev.dropna(subset=['stance'])\n",
        "factoid_test = factoid_test.dropna(subset=['stance'])"
      ],
      "metadata": {
        "id": "UsBPTg8r-rro"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaNs in the stance column\n",
        "print(factoid_train['stance'].isna().sum())\n",
        "print(factoid_dev['stance'].isna().sum())\n",
        "print(factoid_test['stance'].isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr37uD2_-nCD",
        "outputId": "0f4ee3cd-e82a-4129-f0fa-627d185527eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "factoid_model, factoid_vectorizer, factoid_y_test, factoid_y_test_pred = train_baseline_model(\n",
        "    factoid_train, factoid_dev, factoid_test\n",
        ")"
      ],
      "metadata": {
        "id": "yuT5mkVK5-w1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available!\")\n",
        "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU is not available. Check your environment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wDqpxI-WyAy",
        "outputId": "28d24ab9-8019-4543-d611-16de9cfdeca2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available!\n",
            "Device Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "georgetown_train = georgetown_train.dropna(subset=['stance'])\n",
        "georgetown_dev = georgetown_dev.dropna(subset=['stance'])\n",
        "georgetown_test = georgetown_test.dropna(subset=['stance'])"
      ],
      "metadata": {
        "id": "4sngRSlNXNQ5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaNs in the stance column\n",
        "print(georgetown_train['stance'].isna().sum())\n",
        "print(georgetown_dev['stance'].isna().sum())\n",
        "print(georgetown_test['stance'].isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Lp9PjAQXEtN",
        "outputId": "d40a96a0-123a-4edf-d011-df96551a89c9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "georgetown_model, georgetown_vectorizer, georgetown_y_test, georgetown_y_test_pred = train_baseline_model(\n",
        "    georgetown_train, georgetown_dev, georgetown_test\n",
        ")"
      ],
      "metadata": {
        "id": "L0xN6acS5-U-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import evaluate_model, display_evaluation_results\n",
        "\n",
        "# FACTOID Evaluation\n",
        "factoid_labels = ['Negative', 'Neutral', 'Positive']\n",
        "factoid_results = evaluate_model(factoid_y_test, factoid_y_test_pred, factoid_labels)\n",
        "print(\"FACTOID Dataset Evaluation:\")\n",
        "display_evaluation_results(factoid_results, factoid_labels)\n",
        "\n",
        "# Georgetown Evaluation\n",
        "georgetown_labels = ['Negative', 'Neutral', 'Positive']\n",
        "georgetown_results = evaluate_model(georgetown_y_test, georgetown_y_test_pred, georgetown_labels)\n",
        "print(\"\\nGeorgetown Dataset Evaluation:\")\n",
        "display_evaluation_results(georgetown_results, georgetown_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vcGp_G9XD7p",
        "outputId": "867e041a-14f1-4693-9b17-f5321ccb393c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FACTOID Dataset Evaluation:\n",
            "Model Evaluation Metrics\n",
            "-------------------------\n",
            "Accuracy: 0.5783\n",
            "Precision: 0.5418\n",
            "Recall: 0.5783\n",
            "F1-Score: 0.4859\n",
            "\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.59      0.94      0.72    188836\n",
            "     Neutral       0.49      0.13      0.21    117500\n",
            "    Positive       0.45      0.04      0.07     29109\n",
            "\n",
            "    accuracy                           0.58    335445\n",
            "   macro avg       0.51      0.37      0.33    335445\n",
            "weighted avg       0.54      0.58      0.49    335445\n",
            "\n",
            "Confusion Matrix\n",
            "          Negative  Neutral  Positive\n",
            "Negative     15284      838         0\n",
            "Neutral       5211     1158         0\n",
            "Positive         0        0         0\n",
            "\n",
            "Georgetown Dataset Evaluation:\n",
            "Model Evaluation Metrics\n",
            "-------------------------\n",
            "Accuracy: 0.6788\n",
            "Precision: 0.6945\n",
            "Recall: 0.6788\n",
            "F1-Score: 0.6509\n",
            "\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.78      0.41      0.54        34\n",
            "     Neutral       0.66      0.91      0.77        92\n",
            "    Positive       0.70      0.36      0.47        39\n",
            "\n",
            "    accuracy                           0.68       165\n",
            "   macro avg       0.71      0.56      0.59       165\n",
            "weighted avg       0.69      0.68      0.65       165\n",
            "\n",
            "Confusion Matrix\n",
            "          Negative  Neutral  Positive\n",
            "Negative        84        4         0\n",
            "Neutral         25       14         0\n",
            "Positive         0        0         0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1bUyEjm96Rb5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}